---
title: "snapchat_cleaning"
author: "grace douglas"
date: "3/11/2021"
output: html_document
---

libraries
```{r}
library(dplyr)
library(ggplot2)
library(tidyverse)
library(UpSetR)
library(naniar)

```


```{r}
setwd("~/Documents/spring21/stat479/project/snapchat_politics/")
temp <- list.files(pattern="*.csv")
sc <- purrr::map_df(temp, function(x){
  data <- read_csv(x)
  x <- gsub("PoliticalAds_","",x)
  x <- gsub(".csv","",x)
  cbind(year = x, data)
})
dim(sc)
head(sc)

```

*CLEANING*

1. Lets check out the missingness 
```{r, fig.width=10}
gg_miss_var(sc)

#^^^ histogram doesnâ€™t tell us whether missing data tend to co-occur across a few different dimensions. For that, we can count the different missingness patterns.
gg_miss_upset(sc)
#seems like a lot of the missing data is due to geographical targeting and insufficient data collection in regards. We might not care about that in general, QUITE yet.

vis_miss(sc) #LOTS of missing data. must use imputation methods

```


2. Frankly, we dont care about geographical radius quite yet, too specified for us currently. Lets filter out some of the variables. Mainly of them have to do an ad's spatial intent, and recording this a multitude of ways.
```{r}
colnames(sc)

sc = sc %>%
  rename(CurrencyCode = `Currency Code`)  %>%
  rename(RegionsINC = `Regions (Included)`) %>%
  rename(RegionsEXC = `Regions (Excluded)`)

sc = sc %>%
  filter(CurrencyCode == 'USD') %>%
  filter(CountryCode == 'united states') %>%
  select(-c('Radius Targeting (Included)','Radius Targeting (Excluded)', 'Location Categories (Included)', 'Targeting Carrier (ISP)','Location Categories (Excluded)', 'Electoral Districts (Excluded)', 'Electoral Districts (Included)', 'Metros (Excluded)', 'Metros (Included)', 'CreativeProperties', 'ADID', 'CurrencyCode', 'OsType', 'Postal Codes (Included)', 'Postal Codes (Excluded)', 'AdvancedDemographics', 'CandidateBallotInformation', 'Targeting Connection Type', 'Segments', 'Language', 'CountryCode', 'RegionsEXC'))

#check missingness again 
gg_miss_var(sc)

gg_miss_upset(sc)

vis_miss(sc) # better, will be easier to impute

```


Imputation Time! ENDDATE
```{r}

#END DATES
end_dates = sc %>%
  filter(is.na(EndDate))
#View(end_dates) 
#Missing end dates for 2021,2020, 2019, 2018 will be imputed with the end of the next year. I think this is a better assumption then doing 'average' end date.

for (i in 1:nrow(sc)) {
  if (sc$year[i] == '2018' & is.na(sc$EndDate)[i]) {
    sc$EndDate[i] = '2018/12/31'

} else if (sc$year[i] == '2019' & is.na(sc$EndDate)[i]) {
    sc$EndDate[i] = '2019/12/31'

} else if (sc$year[i] == '2020' & is.na(sc$EndDate)[i]) {
    sc$EndDate[i] = '2020/12/31'

} else if (sc$year[i] == '2021' & is.na(sc$EndDate)[i]) {
    sc$EndDate[i] = '2021/12/31'

}
}

#View(sc)
sum(is.na(sc$EndDate)) #works!
#lets also change this variable into a #date# and cut it down to disclude the timestamp 
sc$EndDate = substr(sc$EndDate, 1,10)
sc$EndDate = as.Date(sc$EndDate, '%Y/%m/%d')
#lets go ahead and do the same for StartDate
sc$StartDate = substr(sc$StartDate, 1,10)
sc$StartDate = as.Date(sc$StartDate, '%Y/%m/%d')


#AGE BRACKET, mode imputation? due to categorical class
sum(is.na(sc$AgeBracket))
sc$AgeBracket = as.factor(sc$AgeBracket)
val = unique(sc$AgeBracket[!is.na(sc$AgeBracket)]) 
my_mode = val[which.max(tabulate(match(sc$AgeBracket, val)))] 
sc$AgeBracket[is.na(sc$AgeBracket)] = my_mode  

#GENDER, mode imputation? due to categorical class
sum(is.na(sc$Gender))
sc$Gender = as.factor(sc$Gender)
val_g = unique(sc$Gender[!is.na(sc$Gender)]) 
my_mode_g = val_g[which.max(tabulate(match(sc$Gender, val_g)))] 
sc$Gender[is.na(sc$Gender)] = my_mode_g 

#RegionsINC, for NAs, why dont we assume all regions... otherwise theres no point to the ad
sum(is.na(sc$RegionsINC))
sc$RegionsINC[is.na(sc$RegionsINC)] = 'US'
sc$RegionsINC = as.factor(sc$RegionsINC)

View(sc)

```


 
 Table 1: WHO (advertiser) spends the most money?
```{r}

#grab top 5% of spenders
big_spenders = sc %>%
  group_by(PayingAdvertiserName) %>%
  summarise(total_money = sum(Spend)) %>%
  filter(quantile(total_money, 0.95)<total_money) %>%
  arrange(desc(total_money))

big_spenders

#quick visualization
big_spenders %>% 
  mutate(PayingAdvertiserName = fct_reorder(PayingAdvertiserName, desc(total_money))) %>% 
  ggplot() + 
  geom_col(aes(x = PayingAdvertiserName, y = total_money)) + 
  xlab('Advertiser') + ylab('Total Money Spent') + 
  ggtitle('Big Spenders: WHO is Paying the Most') + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```


Table 2: WHERE (location) is the most money spent?
```{r}

money_states = sc %>%
  group_by(RegionsINC) %>%
  summarise(total_money = sum(Spend)) %>%
  filter(quantile(total_money, 0.95)<total_money) %>%
  arrange(desc(total_money))

money_states

#quick visualization
money_states %>% 
  mutate(RegionsINC = fct_reorder(RegionsINC, desc(total_money))) %>% 
  ggplot() + 
  geom_col(aes(x = RegionsINC, y = total_money)) + 
  xlab('Region') + ylab('Total Money Spent') + 
  ggtitle('Big Spenders: WHERE Are They Paying') + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(label=abbreviate) + 
  scale_y_log10() ## keep in mind! not same as big_spenders vis
```

Deriving - most targeted demographic?
```{r}
target_dem = sc %>%
  group_by(AgeBracket, Gender) %>%
  summarise(total_money = sum(Spend)) %>%
  filter(quantile(total_money, 0.95)<total_money) %>%
  arrange(desc(total_money))
  
target_dem
```



